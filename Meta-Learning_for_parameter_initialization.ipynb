{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning for parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard FashionMNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()    # turn images into Tensor so we can directly use it with our network                             \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " {'T-shirt/top': 6000, 'Trouser': 6000, 'Pullover': 6000, 'Dress': 6000, 'Coat': 6000, 'Sandal': 6000, 'Shirt': 6000, 'Sneaker': 6000, 'Bag': 6000, 'Ankle boot': 6000}\n"
     ]
    }
   ],
   "source": [
    "'''Checking the data set classes and amount of data in each class.'''\n",
    "idx2class = {v: k for k, v in train_set.class_to_idx.items()}\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n",
    "    \n",
    "    for element in dataset_obj:\n",
    "        y_lbl = element[1]\n",
    "        y_lbl = idx2class[y_lbl]\n",
    "        count_dict[y_lbl] += 1\n",
    "            \n",
    "    return count_dict\n",
    "print(\"Distribution of classes: \\n\", get_class_distribution(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five different initialization approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. init with kaiming_normal_\n",
    "torch.nn.init.kaiming_normal_(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_kaiming = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_ones(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "model_kaiming.apply(init_ones)\n",
    "print(model_kaiming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. init with  uniform_: \n",
    "torch.nn.init.uniform_(tensor, a=0.0, b=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_uniform = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "        \n",
    "model_uniform.apply(init_uniform)\n",
    "print(model_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. init with eye_: \n",
    "torch.nn.init.eye_(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_eye = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_zeros(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.eye_(m.weight)\n",
    "        \n",
    "model_eye.apply(init_zeros)\n",
    "print(model_eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. init with  normal_: \n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_normal = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        \n",
    "model_normal.apply(init_normal)\n",
    "print(model_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. init with constant_: \n",
    "torch.nn.init.constant_(tensor, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_constant = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 0.3)\n",
    "        \n",
    "model_constant.apply(init_constant)\n",
    "print(model_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=34, num_workers=0)\n",
    "dataiterator = iter(train_dataloader)\n",
    "images, labels = dataiterator.next()\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "images, labels = next(iter(train_dataloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model_normal(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 0 - Training loss with normal weight: 149.35448463915426\n",
      "Epoch 0 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 0 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 0 - Training loss with eye: 2.315396511250785\n",
      "Epoch 1 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 1 - Training loss with normal weight: 44.55685523846332\n",
      "Epoch 1 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 1 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 1 - Training loss with eye: 2.315396511250785\n",
      "Epoch 2 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 2 - Training loss with normal weight: 30.550186090496375\n",
      "Epoch 2 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 2 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 2 - Training loss with eye: 2.315396511250785\n",
      "Epoch 3 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 3 - Training loss with normal weight: 22.959200171978548\n",
      "Epoch 3 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 3 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 3 - Training loss with eye: 2.315396511250785\n",
      "Epoch 4 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 4 - Training loss with normal weight: 18.170095159278038\n",
      "Epoch 4 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 4 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 4 - Training loss with eye: 2.315396511250785\n",
      "Epoch 5 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 5 - Training loss with normal weight: 14.7102873415177\n",
      "Epoch 5 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 5 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 5 - Training loss with eye: 2.315396511250785\n",
      "Epoch 6 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 6 - Training loss with normal weight: 12.028319613791728\n",
      "Epoch 6 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 6 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 6 - Training loss with eye: 2.315396511250785\n",
      "Epoch 7 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 7 - Training loss with normal weight: 10.073334456705844\n",
      "Epoch 7 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 7 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 7 - Training loss with eye: 2.315396511250785\n",
      "Epoch 8 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 8 - Training loss with normal weight: 8.523362769958993\n",
      "Epoch 8 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 8 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 8 - Training loss with eye: 2.315396511250785\n",
      "Epoch 9 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 9 - Training loss with normal weight: 7.015035874141175\n",
      "Epoch 9 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 9 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 9 - Training loss with eye: 2.315396511250785\n",
      "Epoch 10 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 10 - Training loss with normal weight: 5.514089815964442\n",
      "Epoch 10 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 10 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 10 - Training loss with eye: 2.315396511250785\n",
      "Epoch 11 - Training loss with uniform weight: 42522.54282932011\n",
      "Epoch 11 - Training loss with normal weight: 4.046332451591411\n",
      "Epoch 11 - Training loss with constant weights: 2.3040513098071047\n",
      "Epoch 11 - Training loss with kaiming : 2.585275750254774\n",
      "Epoch 11 - Training loss with eye: 2.315396511250785\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model_normal.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 12\n",
    "m_loss_uniform = []\n",
    "m_loss_normal = []\n",
    "m_loss_constant = []\n",
    "m_loss_kaiming = []\n",
    "m_loss_eye = []\n",
    "for epoch in range(epochs):\n",
    "    run_loss_uniform = 0\n",
    "    run_loss_normal = 0\n",
    "    run_loss_constant = 0\n",
    "    run_loss_kaiming = 0\n",
    "    run_loss_eye = 0\n",
    "    \n",
    "    for images, labels in train_dataloader:\n",
    "        # Flatten FashioMNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad() # set the gradients to zero before starting to do backpropragation\n",
    "        \n",
    "        out_uniform = model_uniform(images)\n",
    "        out_normal = model_normal(images)\n",
    "        out_constant = model_constant(images)\n",
    "        out_kaiming = model_kaiming(images)\n",
    "        out_eye = model_eye(images)\n",
    "        \n",
    "        \n",
    "        loss_uniform = criterion(out_uniform, labels)\n",
    "        loss_normal = criterion(out_normal, labels)\n",
    "        loss_constant = criterion(out_constant, labels)\n",
    "        loss_kaiming = criterion(out_kaiming, labels)\n",
    "        loss_eye = criterion(out_eye, labels)\n",
    "        \n",
    "        # Training model by backprop\n",
    "        loss_uniform.backward()\n",
    "        loss_normal.backward()\n",
    "        loss_constant.backward()\n",
    "        loss_kaiming.backward()\n",
    "        loss_eye.backward()\n",
    "        \n",
    "        # optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_loss_uniform += loss_uniform.item()\n",
    "        run_loss_normal += loss_normal.item()\n",
    "        run_loss_constant += loss_constant.item()\n",
    "        run_loss_kaiming += loss_kaiming.item()\n",
    "        run_loss_eye += loss_eye.item()\n",
    "        \n",
    "        m_loss_uniform.append(loss_uniform.item())\n",
    "        m_loss_normal.append(loss_normal.item())\n",
    "        m_loss_constant.append(loss_constant.item())\n",
    "        m_loss_kaiming.append(loss_kaiming.item())\n",
    "        m_loss_eye.append(loss_eye.item())\n",
    "        \n",
    "    print(\"Epoch {} - Training loss with uniform weight: {}\".format(epoch, run_loss_uniform/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with normal weight: {}\".format(epoch, run_loss_normal/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with constant weights: {}\".format(epoch, run_loss_constant/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with kaiming : {}\".format(epoch, run_loss_kaiming/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with eye: {}\".format(epoch, run_loss_eye/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions(model, loader):\n",
    "    predictions = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        pred = model(images)\n",
    "        predictions = torch.cat(\n",
    "            (predictions, pred)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_model_uniform = get_predictions(model_uniform, train_dataloader)\n",
    "    train_model_normal = get_predictions(model_normal, train_dataloader)\n",
    "    train_model_constant = get_predictions(model_constant, train_dataloader)\n",
    "    train_model_kaiming = get_predictions(model_kaiming, train_dataloader)\n",
    "    train_model_eye = get_predictions(model_eye, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare and report your results using:\n",
    "classification_report and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of correction predictions:  6000\n",
      "accuracy - uniform weights: 0.1\n",
      "No. of correction predictions:  40676\n",
      "accuracy - normal weights: 0.6779333333333334\n",
      "No. of correction predictions:  6985\n",
      "accuracy - constant weights: 0.11641666666666667\n",
      "No. of correction predictions:  4916\n",
      "accuracy kaiming: 0.08193333333333333\n",
      "No. of correction predictions:  6012\n",
      "accuracy eye: 0.1002\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    return predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "print('No. of correction predictions: ', get_accuracy(train_model_uniform, train_set.targets))\n",
    "print('accuracy - uniform weights:', get_accuracy(train_model_uniform, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_normal, train_set.targets))\n",
    "print('accuracy - normal weights:', get_accuracy(train_model_normal, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_constant, train_set.targets))\n",
    "print('accuracy - constant weights:', get_accuracy(train_model_constant, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_kaiming, train_set.targets))\n",
    "print('accuracy kaiming:', get_accuracy(train_model_kaiming, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_eye, train_set.targets))\n",
    "print('accuracy eye:', get_accuracy(train_model_eye, train_set.targets) / len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix with kaiming_normal_ \n",
      "\n",
      "[[ 112    0  340 3011   71    0  424 2042    0    0]\n",
      " [ 102    0  294 2573   34    0 1926 1071    0    0]\n",
      " [ 202    1  404  122   75    0   35 5161    0    0]\n",
      " [  75    0  418 1315   83    0 1925 2184    0    0]\n",
      " [ 104    0  558   78  211    0   88 4961    0    0]\n",
      " [ 195   48 1539 1710   31    0  230 2247    0    0]\n",
      " [ 141    0  339  805  113    0  225 4377    0    0]\n",
      " [ 373    0 1188 1672   15    0  103 2649    0    0]\n",
      " [ 505    9 1136  169   52    0  362 3767    0    0]\n",
      " [  77    2  519 1411   10    0  872 3109    0    0]]\n",
      "\n",
      " Confusion Matrix for Uniform Weights \n",
      "\n",
      "[[   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]]\n",
      "\n",
      " Confusion Matrix with eye_ \n",
      "\n",
      "[[   0    0    1    3    7    7    8  103  157 5714]\n",
      " [   0    0    0    0    1    0    4   54   29 5912]\n",
      " [   0    0    0    0    2    1    3   43   75 5876]\n",
      " [   0    0    0    0    0    0    4   39   35 5922]\n",
      " [   0    0    0    0    0    0    0    6    1 5993]\n",
      " [   0    0    0    0    0    0    0    0    1 5999]\n",
      " [   0    0    0    1   10   15    8  114  164 5688]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]\n",
      " [   0    0    0    2    8    5    0   20    4 5961]\n",
      " [   0    0    0    0    0    0    0    0    0 6000]]\n",
      "\n",
      " Confusion Matrix for Normal Weights \n",
      "\n",
      "[[3760  162  460  534   99  134  727    7  102   15]\n",
      " [ 108 4991  428  268   61   30   85    9   16    4]\n",
      " [ 195   83 4162   57  755   32  513   55  119   29]\n",
      " [ 755  277  379 3584  277  180  446   17   53   32]\n",
      " [ 107   47 2153  144 2072   26 1304   61   62   24]\n",
      " [  16   12    8   16    7 5173   19  441   95  213]\n",
      " [ 951   61 1292  255 1649  103 1441   17  205   26]\n",
      " [   3    4    3    3    1  497    5 4988   46  450]\n",
      " [  47   98  219   86   39  153  171   73 5071   43]\n",
      " [   5    0   11    6    1  236    0  293   14 5434]]\n",
      "\n",
      " Confusion Matrix for Constant Weights \n",
      "\n",
      "[[   0  867 5133    0    0    0    0    0    0    0]\n",
      " [   0 1593 4407    0    0    0    0    0    0    0]\n",
      " [   0  608 5392    0    0    0    0    0    0    0]\n",
      " [   0 1338 4662    0    0    0    0    0    0    0]\n",
      " [   0  345 5655    0    0    0    0    0    0    0]\n",
      " [   0 4928 1072    0    0    0    0    0    0    0]\n",
      " [   0  987 5013    0    0    0    0    0    0    0]\n",
      " [   0 4147 1853    0    0    0    0    0    0    0]\n",
      " [   0  439 5561    0    0    0    0    0    0    0]\n",
      " [   0  306 5694    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n Confusion Matrix with kaiming_normal_ \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_kaiming.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Uniform Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_uniform.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix with eye_ \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_eye.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Normal Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_normal.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Constant Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_constant.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model with \"Normal distribution\" has a better prediction result than all the other 4 models with different init weights.\n",
    "\n",
    "- The model with normal weights has 46553 correction predictions and an accuracy of 0.77 i.e. ~ 77%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
