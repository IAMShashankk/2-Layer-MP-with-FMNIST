{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning for parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard PyTorch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use standard FashionMNIST dataset\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()    # turn images into Tensor so we can directly use it with our network                             \n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of classes: \n",
      " {'T-shirt/top': 6000, 'Trouser': 6000, 'Pullover': 6000, 'Dress': 6000, 'Coat': 6000, 'Sandal': 6000, 'Shirt': 6000, 'Sneaker': 6000, 'Bag': 6000, 'Ankle boot': 6000}\n"
     ]
    }
   ],
   "source": [
    "'''Checking the data set classes and amount of data in each class.'''\n",
    "idx2class = {v: k for k, v in train_set.class_to_idx.items()}\n",
    "def get_class_distribution(dataset_obj):\n",
    "    count_dict = {k:0 for k,v in dataset_obj.class_to_idx.items()}\n",
    "    \n",
    "    for element in dataset_obj:\n",
    "        y_lbl = element[1]\n",
    "        y_lbl = idx2class[y_lbl]\n",
    "        count_dict[y_lbl] += 1\n",
    "            \n",
    "    return count_dict\n",
    "print(\"Distribution of classes: \\n\", get_class_distribution(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network, expand on top of nn.Module\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # nn.Conv2d Applies a 2D convolution over an input signal\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # nn.Linear Applies a linear transformation to the incoming data\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "  # define forward function\n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all hyper params into a OrderedDict, then choose the hyperparameter after multiple epochs\n",
    "params = OrderedDict(\n",
    "    lr = [.01, .001],\n",
    "    batch_size = [100, 1000],\n",
    "    shuffle = [True, False]\n",
    ")\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the hyper-parameters and return a Run namedtuple containing all the \n",
    "# combinations of hyper-parameters\n",
    "class RunBuilder():\n",
    "    @staticmethod \n",
    "    def get_runs(params):\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "    \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five different initialization approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. init with kaiming_normal_\n",
    "torch.nn.init.kaiming_normal_(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_kaiming = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_ones(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "model_kaiming.apply(init_ones)\n",
    "print(model_kaiming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. init with  uniform_: \n",
    "torch.nn.init.uniform_(tensor, a=0.0, b=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_uniform = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_uniform(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "        \n",
    "model_uniform.apply(init_uniform)\n",
    "print(model_uniform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. init with eye_: \n",
    "torch.nn.init.eye_(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_eye = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_zeros(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.eye_(m.weight)\n",
    "        \n",
    "model_eye.apply(init_zeros)\n",
    "print(model_eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. init with  normal_: \n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_normal = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight)\n",
    "        \n",
    "model_normal.apply(init_normal)\n",
    "print(model_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. init with constant_: \n",
    "torch.nn.init.constant_(tensor, val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "\n",
    "model_constant = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 0.3)\n",
    "        \n",
    "model_constant.apply(init_constant)\n",
    "print(model_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=34, num_workers=0)\n",
    "dataiterator = iter(train_dataloader)\n",
    "images, labels = dataiterator.next()\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "images, labels = next(iter(train_dataloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logps = model_normal(images) #log probabilities\n",
    "loss = criterion(logps, labels) #calculate the Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 0 - Training loss with normal weight: 247.31309660460388\n",
      "Epoch 0 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 0 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 0 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 1 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 1 - Training loss with normal weight: 58.41490785879724\n",
      "Epoch 1 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 1 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 1 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 2 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 2 - Training loss with normal weight: 40.6926640062089\n",
      "Epoch 2 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 2 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 2 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 3 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 3 - Training loss with normal weight: 31.537563544967696\n",
      "Epoch 3 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 3 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 3 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 4 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 4 - Training loss with normal weight: 25.992026384694043\n",
      "Epoch 4 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 4 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 4 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 5 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 5 - Training loss with normal weight: 22.15490544183396\n",
      "Epoch 5 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 5 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 5 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 6 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 6 - Training loss with normal weight: 19.33255823063361\n",
      "Epoch 6 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 6 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 6 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 7 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 7 - Training loss with normal weight: 17.159962364972145\n",
      "Epoch 7 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 7 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 7 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 8 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 8 - Training loss with normal weight: 15.439861891965354\n",
      "Epoch 8 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 8 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 8 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 9 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 9 - Training loss with normal weight: 14.01084818252423\n",
      "Epoch 9 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 9 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 9 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 10 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 10 - Training loss with normal weight: 12.786994796940693\n",
      "Epoch 10 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 10 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 10 - Training loss with eye: 2.3144758728340733\n",
      "Epoch 11 - Training loss with uniform weight: 32762.52973065687\n",
      "Epoch 11 - Training loss with normal weight: 11.751496465719455\n",
      "Epoch 11 - Training loss with constant weights: 2.304401360017382\n",
      "Epoch 11 - Training loss with kaiming : 2.3921972043791166\n",
      "Epoch 11 - Training loss with eye: 2.3144758728340733\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model_normal.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 12\n",
    "m_loss_uniform = []\n",
    "m_loss_normal = []\n",
    "m_loss_constant = []\n",
    "m_loss_kaiming = []\n",
    "m_loss_eye = []\n",
    "for epoch in range(epochs):\n",
    "    run_loss_uniform = 0\n",
    "    run_loss_normal = 0\n",
    "    run_loss_constant = 0\n",
    "    run_loss_kaiming = 0\n",
    "    run_loss_eye = 0\n",
    "    \n",
    "    for images, labels in train_dataloader:\n",
    "        # Flatten FashioMNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Training pass\n",
    "        optimizer.zero_grad() # set the gradients to zero before starting to do backpropragation\n",
    "        \n",
    "        out_uniform = model_uniform(images)\n",
    "        out_normal = model_normal(images)\n",
    "        out_constant = model_constant(images)\n",
    "        out_kaiming = model_kaiming(images)\n",
    "        out_eye = model_eye(images)\n",
    "        \n",
    "        \n",
    "        loss_uniform = criterion(out_uniform, labels)\n",
    "        loss_normal = criterion(out_normal, labels)\n",
    "        loss_constant = criterion(out_constant, labels)\n",
    "        loss_kaiming = criterion(out_kaiming, labels)\n",
    "        loss_eye = criterion(out_eye, labels)\n",
    "        \n",
    "        # Training model by backprop\n",
    "        loss_uniform.backward()\n",
    "        loss_normal.backward()\n",
    "        loss_constant.backward()\n",
    "        loss_kaiming.backward()\n",
    "        loss_eye.backward()\n",
    "        \n",
    "        # optimize the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        run_loss_uniform += loss_uniform.item()\n",
    "        run_loss_normal += loss_normal.item()\n",
    "        run_loss_constant += loss_constant.item()\n",
    "        run_loss_kaiming += loss_kaiming.item()\n",
    "        run_loss_eye += loss_eye.item()\n",
    "        \n",
    "        m_loss_uniform.append(loss_uniform.item())\n",
    "        m_loss_normal.append(loss_normal.item())\n",
    "        m_loss_constant.append(loss_constant.item())\n",
    "        m_loss_kaiming.append(loss_kaiming.item())\n",
    "        m_loss_eye.append(loss_eye.item())\n",
    "        \n",
    "    print(\"Epoch {} - Training loss with uniform weight: {}\".format(epoch, run_loss_uniform/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with normal weight: {}\".format(epoch, run_loss_normal/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with constant weights: {}\".format(epoch, run_loss_constant/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with kaiming : {}\".format(epoch, run_loss_kaiming/len(train_dataloader)))\n",
    "    print(\"Epoch {} - Training loss with eye: {}\".format(epoch, run_loss_eye/len(train_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_predictions(model, loader):\n",
    "    predictions = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        pred = model(images)\n",
    "        predictions = torch.cat(\n",
    "            (predictions, pred)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_model_uniform = get_predictions(model_uniform, train_dataloader)\n",
    "    train_model_normal = get_predictions(model_normal, train_dataloader)\n",
    "    train_model_constant = get_predictions(model_constant, train_dataloader)\n",
    "    train_model_kaiming = get_predictions(model_kaiming, train_dataloader)\n",
    "    train_model_eye = get_predictions(model_eye, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare and report your results using:\n",
    "classification_report and confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of correction predictions:  6000\n",
      "accuracy - uniform weights: 0.1\n",
      "No. of correction predictions:  46553\n",
      "accuracy - normal weights: 0.7758833333333334\n",
      "No. of correction predictions:  6000\n",
      "accuracy - constant weights: 0.1\n",
      "No. of correction predictions:  7774\n",
      "accuracy kaiming: 0.12956666666666666\n",
      "No. of correction predictions:  4542\n",
      "accuracy eye: 0.0757\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(predictions, labels):\n",
    "    return predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "\n",
    "print('No. of correction predictions: ', get_accuracy(train_model_uniform, train_set.targets))\n",
    "print('accuracy - uniform weights:', get_accuracy(train_model_uniform, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_normal, train_set.targets))\n",
    "print('accuracy - normal weights:', get_accuracy(train_model_normal, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_constant, train_set.targets))\n",
    "print('accuracy - constant weights:', get_accuracy(train_model_constant, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_kaiming, train_set.targets))\n",
    "print('accuracy kaiming:', get_accuracy(train_model_kaiming, train_set.targets) / len(train_set))\n",
    "\n",
    "print('No. of correction predictions: ',get_accuracy(train_model_eye, train_set.targets))\n",
    "print('accuracy eye:', get_accuracy(train_model_eye, train_set.targets) / len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Confusion Matrix with kaiming_normal_ \n",
      "\n",
      "[[  11    0  710    0 1240   12    0 4020    2    5]\n",
      " [   5    0 1006    0  165   16    0 4808    0    0]\n",
      " [  11    0  335    0  778   16    0 4854    0    6]\n",
      " [   6    0  677    0 1359    6    0 3946    0    6]\n",
      " [  16    0  607    0 1407   22    0 3943    0    5]\n",
      " [ 100    2  190   17  277   77    0 4629  232  476]\n",
      " [  26    0  583    0 1707   20    0 3660    0    4]\n",
      " [  47    0   15    1  102    6    0 5001   70  758]\n",
      " [  17    0  473    0  748   39    0 3903    2  818]\n",
      " [  50    0  179    0  148  110    0 3572 1000  941]]\n",
      "\n",
      " Confusion Matrix for Uniform Weights \n",
      "\n",
      "[[6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]\n",
      " [6000    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      " Confusion Matrix with eye_ \n",
      "\n",
      "[[   0    8    1 3772    4    6   26   49  345 1789]\n",
      " [   0    0    0 4218    1    1   39   26  181 1534]\n",
      " [   0    1    0 4005    1    1   28   12  251 1701]\n",
      " [   0    1    0 4454    0    1   43   19  100 1382]\n",
      " [   0    0    0 5715    0    0    8    1   26  250]\n",
      " [   0    1    0 5996    0    0    1    0    1    1]\n",
      " [   0    1    0 4475    2   20   55   47  329 1071]\n",
      " [   0    0    0 6000    0    0    0    0    0    0]\n",
      " [   0    3    0 5877    4   10   30    4   33   39]\n",
      " [   0    4    0 5995    0    0    1    0    0    0]]\n",
      "\n",
      " Confusion Matrix for Normal Weights \n",
      "\n",
      "[[4560   67  211  352   64   13  641    4   86    2]\n",
      " [  53 5613   43  166   40    5   51    0   29    0]\n",
      " [ 179   47 3998  103  971   10  578    2  110    2]\n",
      " [ 496  179  156 4485  330   16  240    3   94    1]\n",
      " [  58   25  959  207 4205    3  457    1   84    1]\n",
      " [  19    3   21    5    8 5305   22  309   60  248]\n",
      " [1066   46  966  247 1015   12 2467    2  178    1]\n",
      " [   1    0    6    3    1  470    4 4946   42  527]\n",
      " [  59   29  148   78   71   63  111   38 5387   16]\n",
      " [  10    0    4    7    1  130    6  241   14 5587]]\n",
      "\n",
      " Confusion Matrix for Constant Weights \n",
      "\n",
      "[[   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]\n",
      " [   0    0    0    0    0    0    0 6000    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n Confusion Matrix with kaiming_normal_ \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_kaiming.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Uniform Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_uniform.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix with eye_ \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_eye.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Normal Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_normal.argmax(dim=1)))\n",
    "\n",
    "print('\\n Confusion Matrix for Constant Weights \\n')\n",
    "print(confusion_matrix(train_set.targets, train_model_constant.argmax(dim=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model with \"Normal distribution\" has a better prediction result than all the other 4 models with different init weights.\n",
    "\n",
    "- The model with normal weights has 46553 correction predictions and an accuracy of 0.77 i.e. ~ 77%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
